First the Research Engine is going perform research on the client and first it will perform primary research which is scraping their insta, facebook, tiktok, google maps data, google reviews, roleplay chats and calls and then it will also run deep research on them - google maps data, reddit , and every possible data sources and after the research is completed then the demo generator engine will generate a webui demo which will an ai chatbot and voicebot with mock data and mock tools and we will get a developer to test it, and if  any issues in the demo then the developer will fix it and make sure that the demo is working and perfect for showcase and then we will meet with the client in order to showcase them the demo, after this the NDA generator will generate and send a NDA via adobesign or whatever (templatized and automated) and then after the NDA is signed then we will again meet with the client in ordedr to get the use cases and after getting the use cases, the Pricing Model Generator will create a pricing model based on their use case(templatized and automated), and then Proposal & Agreement Draft generator will generate a proposal and agreement draft and it will have a webchat ui and based on our feedback we will be able to update it using the webchatui and also there will be a canvas in the webchatui on the right hand side where we can also edit manually too and then after it is finalised, we will save it and send it to the client and then there will be a PRD Builder Engine which will generate a PRD via a webchat ui, where we will chat with the engine and give all the inputs to it via the chat and the use case of the client and then it will brainstorm with us by cross questioning, thinking edge cases, taking KPIs from the existing implementations for other clients, etc in order to form a better and personalised PRD for the client’s use case, after forming a PRD, it will take a feedback and rework on it unless until we and the client are satisfied. Now this generated PRD will go to the automation engine which is again going to take that via a webchat ui interface and then it will generate a YAML config for that use case in canvas which will open up in the right of the webchat ui interface and it will be editable both via chat feedback and manually editing in the canvas. The YAML config will contain system prompt for the use case, what tools we need for the use case, what tools we have, what tools we don't have, and for the tools which we don't have, the automation engine will automatically create a github issue for it which will contain tool_name, input which the tool will take, output which the tool will produce and other important metadata about tool in the issue and then a developer will work on it manually and when that tool is created and merged then it will automatically get attached to that YAML config via YAML config id and the config will be updated. Now YAML config will also contain what integrations we need for the use case, what integrations we have and what integrations we dont have and for the integrations we dont have, it will again create a github issue automatically and it will contain integration details in the issue and a developer will work on it manually and add the integration to that YAML id and the config will be updated whenever the integration is added and the YAML config will also contain other essential meta data and a unique id. After the YAML config is finalised then the automation engine will load every config of every client and each config will be used to run personalised and custom chatbots for every use case and for multiple clients. Now, what will these chatbots be like? What will be their technical architecture such that they are intelligent and can be powered by using a simple YAML config ? To answer that question, let’s deep dive into the core logic of the automation engine – we are using langgraph for the core workflow logic of the automation engine and it will remain same for every chatbot which will be a two node langraph workflow - an agent node and a tools node as given in this documentation - https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/, ../kishna_diagnostics, ../centurypropertytax and for the voicebot too, the livekit workflow is going to be same, see this codebase - ../kishna_diagnostics/services/voice, the only thing which changes in the core workflow is the system prompt, tools and integrations depending upon the use case. Some examples of tools are fetch_user_flight_information, search_flights, update_ticket_to_new_flight, book_car_rental, update_car_rental, etc. and some examples of integrations are - input channels (instagram, whatsapp, crms, etc), llm integration, database integration ( we will be using supabase for every use case or yaml config), vector db (pinecone for every use case or yaml config), etc.  And thus, this YAML config is going to help us make the system prompt, tools and integrations dynamic as per the use cases we get for every client. Now coming up to the features of each chatbot, what will they do and all? Right now, these voicebots and chatbots will work for automating customer sales and support (inbound and outbound) by conversing via voice(calls) and chat(input channels and output channels) with the client’s customer leads and also tracking the customer leads in the database for each use case/yaml config and also storing the transcript and chats for each and every customer lead for each and every use case/yaml config. This voicebot & chatbot for every use case/yaml config will converse and also try to collect PII (Personal Identifiable Information) from each and every customer lead and store it in the database. It will also do follow ups with the customer leads via scheduled outbound chats and outbound calls. This YAML config should also be configurable to send particular hardcoded template messages and also the voicebot and chatbot will cross sell and up sell to the customer leads too for each and every use case like a salesman does by conversing with them. It will also ask survey questions such as - How do you know about our platform, etc for each use case and for every client. It must also be able to decide when we should transfer to a human agent during voice calls and chat conversations. And also the YAML config should also be configurable in order to enable human agents and our chatbot to chat together with the customer leads. And also, it must also know what to do when a human agent is not available. The voicebot and chatbot for each and every config must also do outbound retargeting based on certain triggers. Now, there should be monitoring for each and every use case/config and it will be done via Monitoring Engine in order to make sure that we know when the voicebot/chatbot is down and for which use case/config, and if the flow changes, if the llm fails, if integrations break or fail, if the llm hallucinates by watching the quality of the responses once it goes live and there should be proactive monitoring too. Now, for customer support for each and every client which we have, they will email us and an AI will talk to them and resolve the issue in the email and if not resolved by ai then the ai will raise a ticket in a dashboard and a human agent will take over. Now, coming over to the customer success engine, It will find customer success metrics for each and every config of each and every client we have. It will calculate metrics, store tFirst the Research Engine is going perform research on the client and first it will perform primary research which is scraping their insta, facebook, tiktok, google maps data, google reviews, roleplay chats and calls and then it will also run deep research on them - google maps data, reddit , and every possible data sources and after the research is completed then the demo generator engine will generate a webui demo which will an ai chatbot and voicebot with mock data and mock tools and we will get a developer to test it, and if  any issues in the demo then the developer will fix it and make sure that the demo is working and perfect for showcase and then we will meet with the client in order to showcase them the demo, after this the NDA generator will generate and send a NDA via adobesign or whatever (templatized and automated) and then after the NDA is signed then we will again meet with the client in ordedr to get the use cases and after getting the use cases, the Pricing Model Generator will create a pricing model based on their use case(templatized and automated), and then Proposal & Agreement Draft generator will generate a proposal and agreement draft and it will have a webchat ui and based on our feedback we will be able to update it using the webchatui and also there will be a canvas in the webchatui on the right hand side where we can also edit manually too and then after it is finalised, we will save it and send it to the client and then there will be a PRD Builder Engine which will generate a PRD via a webchat ui, where we will chat with the engine and give all the inputs to it via the chat and the use case of the client and then it will brainstorm with us by cross questioning, thinking edge cases, taking KPIs from the existing implementations for other clients, etc in order to form a better and personalised PRD for the client’s use case, after forming a PRD, it will take a feedback and rework on it unless until we and the client are satisfied. Now this generated PRD will go to the automation engine which is again going to take that via a webchat ui interface and then it will generate a YAML config for that use case in canvas which will open up in the right of the webchat ui interface and it will be editable both via chat feedback and manually editing in the canvas. The YAML config will contain system prompt for the use case, what tools we need for the use case, what tools we have, what tools we don't have, and for the tools which we don't have, the automation engine will automatically create a github issue for it which will contain tool_name, input which the tool will take, output which the tool will produce and other important metadata about tool in the issue and then a developer will work on it manually and when that tool is created and merged then it will automatically get attached to that YAML config via YAML config id and the config will be updated. Now YAML config will also contain what integrations we need for the use case, what integrations we have and what integrations we dont have and for the integrations we dont have, it will again create a github issue automatically and it will contain integration details in the issue and a developer will work on it manually and add the integration to that YAML id and the config will be updated whenever the integration is added and the YAML config will also contain other essential meta data and a unique id. After the YAML config is finalised then the automation engine will load every config of every client and each config will be used to run personalised and custom chatbots for every use case and for multiple clients. Now, what will these chatbots be like? What will be their technical architecture such that they are intelligent and can be powered by using a simple YAML config ? To answer that question, let’s deep dive into the core logic of the automation engine – we are using langgraph for the core workflow logic of the automation engine and it will remain same for every chatbot which will be a two node langraph workflow - an agent node and a tools node as given in this documentation - https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/, and for the voicebot too, the livekit workflow is going to be same, the only thing which changes in the core workflow is the system prompt, tools and integrations depending upon the use case. Some examples of tools are fetch_user_flight_information, search_flights, update_ticket_to_new_flight, book_car_rental, update_car_rental, etc. and some examples of integrations are - input channels (instagram, whatsapp, crms, etc), llm integration, database integration ( we will be using supabase for every use case or yaml config), vector db (pinecone for every use case or yaml config), etc.  And thus, this YAML config is going to help us make the system prompt, tools and integrations dynamic as per the use cases we get for every client. Now coming up to the features of each chatbot, what will they do and all? Right now, these voicebots and chatbots will work for automating customer sales and support (inbound and outbound) by conversing via voice(calls) and chat(input channels and output channels) with the client’s customer leads and also tracking the customer leads in the database for each use case/yaml config and also storing the transcript and chats for each and every customer lead for each and every use case/yaml config. This voicebot & chatbot for every use case/yaml config will converse and also try to collect PII (Personal Identifiable Information) from each and every customer lead and store it in the database. It will also do follow ups with the customer leads via scheduled outbound chats and outbound calls. This YAML config should also be configurable to send particular hardcoded template messages and also the voicebot and chatbot will cross sell and up sell to the customer leads too for each and every use case like a salesman does by conversing with them. It will also ask survey questions such as - How do you know about our platform, etc for each use case and for every client. It must also be able to decide when we should transfer to a human agent during voice calls and chat conversations. And also the YAML config should also be configurable in order to enable human agents and our chatbot to chat together with the customer leads. And also, it must also know what to do when a human agent is not available. The voicebot and chatbot for each and every config must also do outbound retargeting based on certain triggers. Now, there should be monitoring for each and every use case/config and it will be done via Monitoring Engine in order to make sure that we know when the voicebot/chatbot is down and for which use case/config, and if the flow changes, if the llm fails, if integrations break or fail, if the llm hallucinates by watching the quality of the responses once it goes live and there should be proactive monitoring too. Now, for customer support for each and every client which we have, they will email us and an AI will talk to them and resolve the issue in the email and if not resolved by ai then the ai will raise a ticket in a dashboard and a human agent will take over. Now, coming over to the customer success engine, It will find customer success metrics for each and every config of each and every client we have. It will calculate metrics, store them in db and showcase them in a webui dashboard, in this engine, there will be a customer success agent in which we will feed business context, KPIs, etc of each and every use case/yaml config for each and every client we have and it will generate insights based on that which the human agents will use and follow up with our clients. It can also use insights from one client’s config and use them to help other clients too. Now there will also be a KPI Finder agent which will take data from input channels, get statistics from previous data before we have automated their customer sales and support and after 1 week of operation, it will analyse new data and then calculate KPIs relative to the previous data and then it will plan and A/B test with the customer leads of each and every client and then tweak the system prompt for doing the A/B testing and after 1 week, it will repeat the same process again in order to keep evolving based on the input data from customer leads of every use case/yaml config of every client. We need to do as much automations as possible and humans will be only there to just tie shoelaces (approve, talk to the clients for better human relations, do customer success meetings, meet with the clients, etc.)hem in db and showcase them in a webui dashboard, in this engine, there will be a customer success agent in which we will feed business context, KPIs, etc of each and every use case/yaml config for each and every client we have and it will generate insights based on that which the human agents will use and follow up with our clients. It can also use insights from one client’s config and use them to help other clients too. Now there will also be a KPI Finder agent which will take data from input channels, get statistics from previous data before we have automated their customer sales and support and after 1 week of operation, it will analyse new data and then calculate KPIs relative to the previous data and then it will plan and A/B test with the customer leads of each and every client and then tweak the system prompt for doing the A/B testing and after 1 week, it will repeat the same process again in order to keep evolving based on the input data from customer leads of every use case/yaml config of every client. We need to do as much automations as possible and humans will be only there to just tie shoelaces (approve, talk to the clients for better human relations, do customer success meetings, meet with the clients, etc.)
