# Complete Workflow Automation System

First the Research Engine is going to perform research on the client and first it will perform primary research which is scraping their Instagram, Facebook, TikTok, Google Maps data, Google reviews, and then it will also run deep research on them - Google Maps data, Reddit, and every possible data source. Additionally, a real human will talk to the target customer's sales and support agents to find out loopholes in their existing process across response quality, response time, work hours vs non-work hours coverage, and reverse engineer the kind of workflows they currently run. After the research is completed then the Demo Generator Engine will generate a web UI demo which will be an AI chatbot and voicebot with mock data and mock tools and we will get a developer to test it, and if there are any issues in the demo then the developer will fix it and make sure that the demo is working and perfect for showcase. Then we will meet with the client in order to showcase them the demo through our sales person. After this meeting, if the client agrees for a pilot, then the NDA Generator will generate and send an NDA via AdobeSign or similar platform (templatized and automated) - the NDA will be based on the client's business type and sent only after the client meeting and pilot agreement.
After the NDA is signed then we will again meet with the client in order to get the use cases and objectives, and after getting the use cases, the Pricing Model Generator will create a pricing model based on their use case (templatized and automated) - this will include Ashay's financial cost module with pricing tiers. Then Proposal & Agreement Draft Generator will generate a proposal and agreement draft and it will have a webchat UI and based on our feedback we will be able to update it using the webchat UI and also there will be a canvas in the webchat UI on the right hand side where we can also edit manually too. After it is finalized, we will save it and send it to the client.
Then there will be a PRD Builder Engine which will be smart and dynamic and will generate a PRD via a webchat UI, where we will chat with the engine and give all the inputs to it via the chat including the use case and objectives of the client. This engine will take intelligence from the objectives of the client, integration data sources, and our experience with other customers (village knowledge - knowledge of what's working for other customers). It will brainstorm with us by cross questioning, thinking edge cases, taking KPIs from the existing implementations for other clients. It will design A/B flows based on the objective using data sources for personalization and improvement. It will plan follow ups, it will plan persuasive scripts based on each A/B test and see what existing and extra data sources it can use to measure and personalize sales workflows or support workflows.
The PRD Builder will also suggest new objectives to business that they did not even ask for based on our experience to be profitable or cost saving to customers, like cross sell, upsell, and in support - rapport building for cross sell, and running product/service surveys. It will offer new business objectives and create new revenue centers and cost saving centers. This will be driven by village knowledge (knowledge of what's working for other customers) or AI intelligence based on customer's business and audience.
The PRD will also cover how the integration of workflow will work - when to escalate to human, whether it will work alongside humans or independently, how will it integrate to the ecosystem of tools they are using, what tickets at what trigger should be created in client's internal tools. For each objective, a couple of A/B flows will be designed and for each A/B flow, KPIs will be measured. For this, a BASELINE needs to be determined - a reality check of what data we have, what data the client has and what extra they are willing to share will be done right there in the PRD creation process. We will inform the client that if they add this data, the analytics can get better and the agent too (we expect few to do this so this can be a continuing activity where customer success tries to convince to give more data and possibly cross-sell SmartPlaybooks to the user, just like we did with GoFix/GoGizmo).
Log events to track uptime across the whole flow will be decided during PRD creation. Sprint by sprint planning will be done with the goal of 95% automation in 12 months but we start step by step. We will decide what will: 1) AI platform team automate sprint by sprint, 2) where will humans come in and just tie shoelaces in the conveyor built, 3) where platform engineers will step in to solve critical bugs, platform downtimes, create RCA and implement solutions to mitigate forever.
After forming a PRD, it will take feedback and rework on it until we and the client are satisfied. Now this generated PRD will go to the Automation Engine which is again going to take that via a webchat UI interface and then it will generate a YAML config for that use case in canvas which will open up on the right of the webchat UI interface and it will be editable both via chat feedback and manually editing in the canvas.
The YAML config will contain system prompt for the use case, what tools we need for the use case, what tools we have, what tools we don't have, and for the tools which we don't have, the automation engine will automatically create a GitHub issue for it which will contain tool_name, input which the tool will take, output which the tool will produce and other important metadata about tool in the issue and then a developer will work on it manually and when that tool is created and merged then it will automatically get attached to that YAML config via YAML config id and the config will be updated.
Now YAML config will also contain what integrations we need for the use case, what integrations we have and what integrations we don't have and for the integrations we don't have, it will again create a GitHub issue automatically and it will contain integration details in the issue and a developer will work on it manually and add the integration to that YAML id and the config will be updated whenever the integration is added and the YAML config will also contain other essential metadata and a unique id.
After the YAML config is finalized then the automation engine will load every config of every client and each config will be used to run personalized and custom chatbots for every use case and for multiple clients. Now, what will these chatbots be like? What will be their technical architecture such that they are intelligent and can be powered by using a simple YAML config? To answer that question, let's deep dive into the core logic of the automation engine â€“ we are using LangGraph for the core workflow logic of the automation engine and it will remain same for every chatbot which will be a two node LangGraph workflow - an agent node and a tools node as given in this documentation - https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/, ../kishna_diagnostics, ../centurypropertytax and for the voicebot too, the LiveKit workflow is going to be same, see this codebase - ../kishna_diagnostics/services/voice, the only thing which changes in the core workflow is the system prompt, tools and integrations depending upon the use case. Some examples of tools are fetch_user_flight_information, search_flights, update_ticket_to_new_flight, book_car_rental, update_car_rental, etc. and some examples of integrations are - input channels (Instagram, WhatsApp, CRMs, etc), LLM integration, database integration (we will be using Supabase for every use case or YAML config), vector db (Pinecone for every use case or YAML config), etc. And thus, this YAML config is going to help us make the system prompt, tools and integrations dynamic as per the use cases we get for every client.
Now coming up to the features of each chatbot, what will they do and all? Right now, these voicebots and chatbots will work for automating customer sales and support (inbound and outbound) by conversing via voice (calls) and chat (input channels and output channels) with the client's customer leads and also tracking the customer leads in the database for each use case/YAML config and also storing the transcript and chats for each and every customer lead for each and every use case/YAML config. This voicebot & chatbot for every use case/YAML config will converse and also try to collect PII (Personal Identifiable Information) from each and every customer lead and store it in the database. It will also do follow ups with the customer leads via scheduled outbound chats and outbound calls. This YAML config should also be configurable to send particular hardcoded template messages and also the voicebot and chatbot will cross sell and up sell to the customer leads too for each and every use case like a salesman does by conversing with them. It will also ask survey questions such as - How do you know about our platform, etc for each use case and for every client. It must also be able to decide when we should transfer to a human agent during voice calls and chat conversations. And also the YAML config should also be configurable in order to enable human agents and our chatbot to chat together with the customer leads. And also, it must also know what to do when a human agent is not available. The voicebot and chatbot for each and every config must also do outbound retargeting based on certain triggers.
Now, there should be monitoring for each and every use case/config and it will be done via Monitoring Engine in order to make sure that we know when the voicebot/chatbot is down and for which use case/config, and if the flow changes, if the LLM fails, if integrations break or fail, if the LLM hallucinates by watching the quality of the responses once it goes live and there should be proactive monitoring too. During creation of the PRD and its integrations, it will be discussed what all events to track and if any event is breaking across LLM tools, LLM provider, integrations, database - it will create an incident and will alert Platform uptime engineers. Client will be informed and also the resolution report with an RCA will be made to prevent such future problems. If it breaks client SLA, we will refund the time it was down in the monthly billing if there is a minimum clause.
Now, for customer support for each and every client which we have, they will email us and an AI will talk to them and resolve the issue in the email and if not resolved by AI then the AI will raise a ticket in a dashboard and a human agent will take over. We need to create support documentation which creates support matrix as what are common support questions and how to solve them - let AI do it and a human approve it. For complex queries which is documented as complex or not documented at all, we need to escalate to humans. We need to copy Freshworks support system and the way they have platform experts - we need to reverse engineer this by talking to Rahul.
Now, coming over to the Customer Success Engine and onboarding to CS handoff process. Initially (first few weeks after customer goes live) customer success needs to be proactive and manually overseen by a customer success expert who sample checks for the flow and see if it's performing, where it's breaking and makes fixes with the onboarding team. There's a handoff process that will be manual but overseen by AI to make sure it's happening properly. We have a process to make sure the human is doing this job and how we should be doing the job and update the engine of his work, discoveries, optimizations and meetings with clients. Initially flow updates will be higher until the AI flow is well settled into the company.
The Customer Success Engine will find customer success metrics for each and every config of each and every client we have. It will calculate metrics, store them in db and showcase them in a web UI dashboard. In this engine, there will be a customer success agent in which we will feed business context, KPIs, etc of each and every use case/YAML config for each and every client we have and it will generate insights based on that which the human agents will use and follow up with our clients. It can also use insights from one client's config and use them to help other clients too. We can use original ideas from SmartPlaybooks like analyzing chat transcripts of customers of client, CRM data and ask for more data from client to help better give solutions to client.
An ontology needs to be created with KPIs, A/B of the flow and customer meeting notes. Every quarter a customer success meeting is held and new ideas are discussed to optimize the flow. Sometimes insights from customers of our client that are not related to our product can be given. All of this should be agentic and a PPT should be auto-generated. CS human agent is a puppet who can communicate well. These agentic workflows need to separately be made initially by AI and approved by senior customer success managers. Deepak, me and Rahul need to plan the specifics of this. We can talk to Claude also to give us a basic understanding.
Now there will also be a KPI Finder Agent which will take data from input channels, get statistics from previous data before we have automated their customer sales and support (baseline) and after 1 week of operation, it will analyze new data and then calculate KPIs relative to the previous data and then it will plan and A/B test with the customer leads of each and every client and then tweak the system prompt for doing the A/B testing and after 1 week, it will repeat the same process again in order to keep evolving based on the input data from customer leads of every use case/YAML config of every client.
We need to make a CRM flow which keeps changing as automation keeps increasing to track, assign, review and get insights into how different humans in our team, then client's team and automation collaborates across the lifecycle of the client starting from sales, onboarding, support and success.
We also need to plan our internal KPIs - also find out how to measure it, and make review and improvement frameworks. Both for our automation AI agents and real humans across the client lifecycle. This includes correct judgments with lesser iterations, reliable performance, uptime, quick resolutions, user growth and retention.
How will this work over sprints? Implement modularity across the whole automation cycle, explain technical challenges each module and fit into sprints for platform team. As more sprints release more automations happen. For challenges in modules we can engage top experts to help architect it so modularize the whole thing. If we can't afford to hire very experienced talent for the whole platform, we can at least hire for that key module we can't internally optimize or crack.
There are few ideas on fine tuning for future modules where we can engage experts to help:
When we have data can we fine tune SOTA models that is the best for the task already for onboarding agents and customer success agents. I'm sure they can increase performance.


Also think of automated fine tuning and RL for each client as we go forward. So each of our client can have their own LLM managed by us that keeps getting updated: A) if they gather more data, B) if a better new open source model releases (we train GPT-4 OS, then GPT-5 OS comes). This should decrease costs and increase reliability both by 30 to 50% on each of the two, then it makes sense.


We need to do as much automations as possible and humans will be only there to just tie shoelaces (approve, talk to the clients for better human relations, do customer success meetings, meet with the clients, etc.)
```

I want you to generate a technical architecture document in order to build this workflow automation using microservices architecture with the required and necessary techstack and ecosystem, use web search mcp and context7 mcp to research about techstack and ecosystem such as agentic frameworks(langgraph/langchain), rag, graphrag, mcp, context engineering, livekit server, livekit sip etc and also what is being used currently in various ai agents and automations based startups, platforms, services, etc. (example source - yc backed startups,anthropic etc) and also read and do a deep and detailed meta analysis of this research doc - @RESEARCH.md. You need to research about how other ai based startups/companies are doing context engineering, if they are using rag or graph rag, are they using their own frameworks or opensource frameworks, their software architecture design, etc  and based on all this meta analysis and  information, you need to design the most optimal software architecture required for building the automations stated in the workflow automation doc and also meta analyse the codebase(relative paths mentioned) and also the links mentioned in the workflow automation doc and also remember that voicebot has a different code than chatbot, it uses livekit server and livekit SIP and also we will be using gemini-2.5-pro and gemini-2.5-flash for llms and GCP for cloud and also make sure the you use updated knowledge from web search mcp for everything, not outdated stuff should me present in the doc, validate everything use web search or context7 mcp
